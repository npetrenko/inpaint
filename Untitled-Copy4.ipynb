{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.12.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_learning_phase = tf.placeholder(tf.bool, [], \"disc_learning_phase\")\n",
    "dec_learning_phase = tf.placeholder(tf.bool, [], \"dec_learning_phase\")\n",
    "\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(\"//cpu:0\"):\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "\n",
    "    true_data = train_dataset.concatenate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x, y):\n",
    "    x = tf.cast(x, tf.float32)\n",
    "    x -= 125.\n",
    "    x /= 125.\n",
    "    x = tf.expand_dims(x, axis=-1)\n",
    "#     x = tf.image.resize_images(x, (32,32))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(\"//cpu:0\"):\n",
    "    train_dataset = train_dataset.shuffle(x_train.shape[0], reshuffle_each_iteration=False)\\\n",
    "    .shuffle(1024).batch(2048).map(preprocess)\n",
    "    \n",
    "    test_dataset = test_dataset.shuffle(x_test.shape[0], reshuffle_each_iteration=False)\\\n",
    "    .batch(256).map(preprocess)\n",
    "\n",
    "    handle = tf.placeholder(tf.string, [])\n",
    "    iterator = tf.data.Iterator.from_string_handle(handle, \n",
    "                                                   train_dataset.output_types, \n",
    "                                                   train_dataset.output_shapes)\n",
    "\n",
    "    train_iterator = train_dataset.make_initializable_iterator()\n",
    "    test_iterator = test_dataset.make_initializable_iterator()\n",
    "    \n",
    "    next_elt = iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_handle = sess.run(train_iterator.string_handle())\n",
    "test_handle = sess.run(test_iterator.string_handle())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'IteratorGetNext:0' shape=(?, 28, 28, 1) dtype=float32>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_elt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(train_iterator.initializer)\n",
    "sess.run(test_iterator.initializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.layers as tfl\n",
    "import tensorflow.keras.layers as kerasl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 32\n",
    "class Model:\n",
    "    def __init__(self, learning_phase):\n",
    "        self.summaries = []\n",
    "        self.learning_phase = learning_phase\n",
    "        \n",
    "    def batchnorm(self):\n",
    "        return lambda x: tfl.batch_normalization(x, training=self.learning_phase, momentum=0.5)\n",
    "    \n",
    "    def dropout(self, rate=0.5):\n",
    "        return lambda x: tfl.dropout(x, rate=rate, training=self.learning_phase)\n",
    "    \n",
    "    def add_summary(self, summ):\n",
    "        self.summaries += [summ]\n",
    "        \n",
    "    def get_merged_summaries(self):\n",
    "        return tf.summary.merge(self.summaries)\n",
    "    \n",
    "class DecoderModel(Model):\n",
    "    def __init__(self, learning_phase):\n",
    "        super().__init__(learning_phase)\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, inp):\n",
    "        with tf.variable_scope(\"decoder\", reuse=tf.AUTO_REUSE) as model_scope:\n",
    "            x = tfl.Dense(64)(inp)\n",
    "            x = self.batchnorm()(x)\n",
    "            x = tf.nn.leaky_relu(x)\n",
    "            \n",
    "            x = tfl.Dense(256)(x)\n",
    "            x = self.batchnorm()(x)\n",
    "            x = tf.nn.leaky_relu(x)\n",
    "            \n",
    "            x = tfl.Dense(512)(x)\n",
    "#             x = self.batchnorm()(x)\n",
    "            x = tf.nn.leaky_relu(x)\n",
    "            \n",
    "            x = tfl.Dense(28*28, activation=tf.nn.tanh)(x)\n",
    "            x = tf.reshape(x, [-1, 28, 28, 1])\n",
    "        \n",
    "        self.variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=model_scope.name)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def get_variables(self):\n",
    "        return self.variables\n",
    "    \n",
    "decoder_model = DecoderModel(dec_learning_phase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder_model(tf.zeros([100, latent_dim]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscriminatorModel(Model):\n",
    "    def __init__(self, learning_phase):\n",
    "        super().__init__(learning_phase)\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, inp):\n",
    "        with tf.variable_scope(\"discriminator\", reuse=tf.AUTO_REUSE) as model_scope:\n",
    "            x = tfl.Flatten()(inp)\n",
    "            x = tfl.Dense(512)(x)\n",
    "            x = self.batchnorm()(x)\n",
    "            x = tf.nn.leaky_relu(x)\n",
    "            \n",
    "            x = self.dropout(0.25)(x)\n",
    "            \n",
    "            x = tfl.Dense(256, activation=\"relu\")(x)\n",
    "            x = self.batchnorm()(x)\n",
    "            x = tf.nn.leaky_relu(x)\n",
    "            \n",
    "            x = self.dropout(0.25)(x)\n",
    "            \n",
    "            x = tfl.Dense(1, activation=None)(x)\n",
    "            \n",
    "        self.variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=model_scope.name)\n",
    "        return x\n",
    "    \n",
    "    def get_variables(self):\n",
    "        return self.variables\n",
    "    \n",
    "discriminator_model = DiscriminatorModel(disc_learning_phase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"concat:0\", shape=(?, 28, 28, 1), dtype=float32)\n",
      "Tensor(\"Log:0\", shape=(?, 1), dtype=float32)\n",
      "Tensor(\"concat_1:0\", shape=(?, 1), dtype=float32)\n",
      "Tensor(\"Cast_1:0\", shape=(?, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def build_gan_losses():\n",
    "    num_true = tf.shape(next_elt)[0]\n",
    "\n",
    "    decoder_output = decoder_model(tf.random_normal([num_true,latent_dim]))\n",
    "    num_false = tf.shape(decoder_output)[0]\n",
    "    \n",
    "    disc_input = tf.concat([decoder_output, next_elt], axis=0)\n",
    "    print(disc_input)\n",
    "    \n",
    "    disc_logits = discriminator_model(disc_input)\n",
    "\n",
    "    false_samples_logp = tf.log(tf.nn.sigmoid(disc_logits[:num_false]))\n",
    "    print(false_samples_logp)\n",
    "    \n",
    "    dec_gain = tf.reduce_mean(false_samples_logp)\n",
    "    \n",
    "    labels = tf.concat([tf.zeros([num_false,1]), \n",
    "                        tf.ones([num_true,1])], axis=0)\n",
    "    print(labels)\n",
    "                  \n",
    "    disc_loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=labels, logits=disc_logits)\n",
    "    disc_gain = -tf.reduce_mean(disc_loss)\n",
    "    \n",
    "    def accuracy(labels, preds):\n",
    "        tmp = tf.cast(tf.equal(labels, tf.cast(preds, tf.float32)), tf.float32)\n",
    "        print(tmp)\n",
    "        return tf.reduce_mean(tmp)\n",
    "    \n",
    "    accuracy = accuracy(labels, tf.nn.sigmoid(disc_logits)>0.5)\n",
    "    discriminator_model.add_summary(tf.summary.scalar(\"disc_acc\", accuracy))\n",
    "    discriminator_model.add_summary(tf.summary.scalar(\"disc_loss\", -disc_gain))\n",
    "    \n",
    "    decoder_model.add_summary(tf.summary.scalar(\"dec_loss\", -dec_gain))\n",
    "        \n",
    "    return dec_gain, disc_gain\n",
    "\n",
    "dec_gain, disc_gain = build_gan_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_gan_update():\n",
    "    opt = tf.train.AdamOptimizer(learning_rate=0.0002)\n",
    "\n",
    "#     def build_update(gain, variables):\n",
    "#         grads = tf.gradients(gain, variables)\n",
    "#         grads = map(lambda x: tf.clip_by_value(x, -5., 5.), grads)\n",
    "#         grads_and_vars = [(x,y) for x,y in zip(grads, variables) if x != None]\n",
    "#         return opt.apply_gradients(grads_and_vars)\n",
    "    \n",
    "#     decoder_upd = build_update(dec_gain, decoder_model.get_variables())\n",
    "#     disc_upd = build_update(disc_gain, discriminator_model.get_variables())\n",
    "    decoder_upd = opt.minimize(-dec_gain, var_list=decoder_model.get_variables())\n",
    "    disc_upd = opt.minimize(-disc_gain, var_list=discriminator_model.get_variables())\n",
    "    return (decoder_upd, disc_upd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_summaries = decoder_model.get_merged_summaries()\n",
    "disc_summaries = discriminator_model.get_merged_summaries()\n",
    "\n",
    "def get_dir():\n",
    "    import os\n",
    "    base_dir = \"/home/nikita/tmp/gan_logs/\"\n",
    "    for ix in range(1000):\n",
    "        cand = os.path.join(base_dir, \"dense_dbn_0.2_\" + str(ix))\n",
    "        if not os.path.exists(cand):\n",
    "            return cand\n",
    "        \n",
    "writer = tf.summary.FileWriter(get_dir())\n",
    "global_step = tf.train.get_or_create_global_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_upd, disc_upd = build_gan_update()\n",
    "global_step_upd = global_step.assign_add(1)\n",
    "\n",
    "decoder_upd = [decoder_upd, global_step_upd]\n",
    "disc_upd = [disc_upd, global_step_upd]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_initializer = tf.global_variables_initializer()\n",
    "sess.run(variable_initializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"ExpandDims:0\", shape=(1, 168, 168, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "num_images = 6\n",
    "\n",
    "noise = np.random.normal(size=[num_images,latent_dim]).astype(np.float32)\n",
    "ggg = decoder_model(tf.constant(noise))\n",
    "\n",
    "class TBImUploader:\n",
    "    def __init__(self, num_rows):\n",
    "        self.num_rows = num_rows\n",
    "        self.num_images = num_rows**2\n",
    "        noise = np.random.normal(size=[self.num_images, latent_dim]).astype(np.float32)\n",
    "        \n",
    "        model_output = decoder_model(tf.constant(noise))\n",
    "        flattened = self.flatten(model_output)\n",
    "        self.summary = tf.summary.image(\"img\", flattened)\n",
    "        \n",
    "    def flatten(self, img_t):\n",
    "        exp = tf.reshape(img_t, [self.num_rows,self.num_rows*28,28,1])\n",
    "        exp = [exp[i] for i in range(self.num_rows)]\n",
    "        exp = tf.concat(exp, axis=1)\n",
    "        exp = tf.expand_dims(exp, 0)\n",
    "        print(exp)\n",
    "        return exp\n",
    "    \n",
    "    def post_summary(self):\n",
    "        sm = self.summary.eval({dec_learning_phase: False})\n",
    "        writer.add_summary(sm, global_step.eval())\n",
    "        \n",
    "im_summarizer = TBImUploader(6)\n",
    "    \n",
    "def show_pics():\n",
    "    arr = sess.run(ggg, {dec_learning_phase: False})\n",
    "    arr = np.concatenate([arr[i] for i in range(arr.shape[0])], axis=1)\n",
    "    plt.imshow(arr[...,0])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = tf.random_normal([10,latent_dim])\n",
    "gened = decoder_model(noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tt = sess.run(gened, {dec_learning_phase: False})\n",
    "# from matplotlib import pyplot as plt\n",
    "\n",
    "# for i in range(tt.shape[0]):\n",
    "#     plt.imshow(tt[i][...,0])\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "dec_dic = {handle: train_handle, dec_learning_phase: True, disc_learning_phase: False}\n",
    "disc_dic = {handle: train_handle, dec_learning_phase: False, disc_learning_phase: True}\n",
    "\n",
    "# dec_dic = {handle: train_handle, dec_learning_phase: False, disc_learning_phase: False}\n",
    "# disc_dic = {handle: train_handle, dec_learning_phase: False, disc_learning_phase: False}\n",
    "\n",
    "for epoch in range(100000):\n",
    "    if epoch % 1 == 0:\n",
    "        print(epoch)\n",
    "        show_pics()\n",
    "        im_summarizer.post_summary()\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            if random.random() < 1.:\n",
    "                _, summ = sess.run([disc_upd, disc_summaries], disc_dic)\n",
    "                writer.add_summary(summ, global_step.eval())\n",
    "            \n",
    "            if random.random() < (0 if epoch < 1 else .2):\n",
    "                _, summ = sess.run([decoder_upd, dec_summaries], dec_dic)\n",
    "                writer.add_summary(summ, global_step.eval())\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        sess.run(train_iterator.initializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(variable_initializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
